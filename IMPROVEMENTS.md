# Whistx v2 - ç²¾åº¦æ”¹å–„ï¼†UIåˆ·æ–°ãƒ—ãƒ©ãƒ³

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

NVIDIA **H200** GPUã®è¨ˆç®—è³‡æºã‚’æ´»ç”¨ã—ã€**é«˜ç²¾åº¦**ã‹ã¤**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ **ãªä¼šè­°æ–‡å­—èµ·ã“ã—ã¨ã€**vLLM**ã«ã‚ˆã‚‹ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã®å³æ™‚åˆ†æï¼ˆè¦ç´„ãƒ»ã‚¿ã‚¹ã‚¯æŠ½å‡ºï¼‰ã‚’å®Ÿç¾ã™ã‚‹ã€‚

**é‡ç‚¹é ˜åŸŸ:**
1. **èªè­˜ç²¾åº¦ã®æœ€å¤§åŒ–** - Whisper Large-v3 (Float16) + H200æœ€é©åŒ–
2. **ãƒ¢ãƒ€ãƒ³UIã¸ã®åˆ·æ–°** - React (Vite) + Tailwind CSS
3. **å®Œå…¨ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹** - vLLM + ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆAPIä¸è¦ãƒ»ãƒ‡ãƒ¼ã‚¿å¤–æµãªã—ï¼‰

---

## ç›®æ¬¡

1. [ç²¾åº¦æ”¹å–„ï¼ˆASRã‚¨ãƒ³ã‚¸ãƒ³ï¼‰](#1ç²¾åº¦æ”¹å–„asrã‚¨ãƒ³ã‚¸ãƒ³)
2. [UI/UXæ”¹å–„](#2uiuxæ”¹å–„)
3. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#3ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
4. [ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆï¼ˆvLLMï¼‰](#4ãƒ­ãƒ¼ã‚«ãƒ«llmçµ±åˆvllm)
5. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ·æ–°](#5ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ·æ–°)
6. [å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—](#6å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—)

---

## 1. ç²¾åº¦æ”¹å–„ï¼ˆASRã‚¨ãƒ³ã‚¸ãƒ³ï¼‰

### 1.1 âœ… Whisper Large-v3 (Float16) ã¸ã®ç§»è¡Œ

**ç¾çŠ¶:**
```python
# server/asr_backends.py:82-92
# Parakeet-CTCï¼ˆæ—¥æœ¬èªç‰¹åŒ–ï¼‰ã‚’ä½¿ç”¨
model = EncDecCTCModel.from_pretrained(model_name="nvidia/stt_ja_parakeet_ctc_1.1b")
```

Parakeet-CTCã¯æ—¥æœ¬èªã«ç‰¹åŒ–ã—ã¦ã„ã‚‹ãŒã€è‹±èªã‚„å¤šè¨€èªå¯¾å¿œãŒå¼±ã„ã€‚

**æ”¹å–„æ¡ˆ:**
```python
# backend/transcription.py
from faster_whisper import WhisperModel

class H200Transcriber:
    def __init__(self):
        # H200å‘ã‘æœ€é©è¨­å®š
        self.model = WhisperModel(
            "large-v3",                    # æœ€å¤§ç²¾åº¦ãƒ¢ãƒ‡ãƒ«
            device="cuda",
            compute_type="float16",        # H200ã§æœ€é©ãªç²¾åº¦
            device_index=0,
            cpu_threads=8,
            num_workers=4
        )
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- æ—¥æœ¬èªWER: **ã€œ15%** å‘ä¸Šï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å€¤ï¼‰
- è‹±èªå¯¾å¿œ: ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ¬ãƒ™ãƒ«
- å¤šè¨€èªæ··åœ¨: 99è¨€èªã«å¯¾å¿œ

**å„ªå…ˆåº¦:** **æœ€å„ªå…ˆ**

---

### 1.2 âœ… Initial Promptã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç¶­æŒ

**ç¾çŠ¶:**
å„ç™ºè©±ãŒç‹¬ç«‹ã—ã¦å‡¦ç†ã•ã‚Œã€ä¼šè©±ã®æ–‡è„ˆãŒè€ƒæ…®ã•ã‚Œãªã„ã€‚

**æ”¹å–„æ¡ˆ:**
```python
def transcribe_with_context(self, audio: bytes, history: List[str]) -> str:
    # ç›´è¿‘ã®è­°äº‹éŒ²ã‚’initial_promptã¨ã—ã¦æ¸¡ã™
    context = "\n".join(history[-10:])  # ç›´è¿‘10ç™ºè©±
    segments, info = self.model.transcribe(
        audio,
        language="ja",
        initial_prompt=context,           # æ–‡è„ˆã‚’è€ƒæ…®
        beam_size=12,                     # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã§ç²¾åº¦å‘ä¸Š
        vad_filter=True,
        word_timestamps=True
    )
    return " ".join(seg.text for seg in segments)
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- å°‚é–€ç”¨èªã®èªè­˜ç²¾åº¦å‘ä¸Š
- åŒéŸ³ç•°ç¾©èªã®èª¤èªè­˜ä½æ¸›
- ä¼šè©±ã®æµã‚Œã«å³ã—ãŸå¥èª­ç‚¹

**å„ªå…ˆåº¦:** **é«˜**

---

### 1.3 âœ… VADï¼ˆVoice Activity Detectionï¼‰ã®é«˜åº¦åŒ–

**ç¾çŠ¶:**
```python
# server/transcribe_worker.py:252-254
# Silero VADã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
wav_t = torch.from_numpy(wav).to(self.device)
ts_list = get_speech_ts(wav_t, model, threshold=0.5)
```

é–¾å€¤ãŒå›ºå®šã§ã€ãƒã‚¤ã‚ºç’°å¢ƒã§èª¤æ¤œçŸ¥ã€‚

**æ”¹å–„æ¡ˆ:**
```python
class AdaptiveVAD:
    def __init__(self):
        self.silero_model = get_silero_model()
        self.threshold = 0.5
        self.noise_floor = 0.0

    def auto_calibrate(self, noise_sample: np.ndarray):
        # ç’°å¢ƒãƒã‚¤ã‚ºã«å¿œã˜ã¦é–¾å€¤ã‚’è‡ªå‹•èª¿æ•´
        self.noise_floor = np.mean(np.abs(noise_sample))
        self.threshold = 0.3 + (self.noise_floor * 2)

    def detect_speech(self, audio_chunk: bytes) -> bool:
        vad_score = self.silero_model(audio_chunk)
        return vad_score > self.threshold
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- ã‚¨ã‚¢ã‚³ãƒ³ç­‰ã®ç’°å¢ƒãƒã‚¤ã‚ºã«ã‚ˆã‚‹èª¤æ¤œçŸ¥ã‚’90%å‰Šæ¸›
- å°å£°ã®å–ã‚Šã“ã¼ã—ã‚’ä½æ¸›

**å„ªå…ˆåº¦:** **é«˜**

---

### 1.4 ãƒ›ãƒƒãƒˆãƒ¯ãƒ¼ãƒ‰ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®å¼·åŒ–

**ç¾çŠ¶:**
```python
# server/hotwords.py:60-78
# ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã§å…¨æ¢ç´¢ï¼ˆè¨ˆç®—é‡å¤§ï¼‰
while i < len(out):
    for wlen in range(max_len, min_len - 1, -1):
        # ...
```

**æ”¹å–„æ¡ˆ:**
```python
# Aho-Corasickæ³•ã§é«˜é€ŸåŒ–
import ahocorasick

class HotwordBooster:
    def __init__(self, words: List[str]):
        self.automaton = ahocorasick.Automaton()
        for word in words:
            self.automaton.add_word(word, word)
        self.automaton.make_automaton()

    def boost(self, text: str) -> Dict[str, int]:
        # O(n)ã§æ¤œå‡º
        return {word: count for _, word in self.automaton.iter(text)}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- å°‚é–€ç”¨èªã®èªè­˜ç‡: **ã€œ30%** å‘ä¸Š
- è¨ˆç®—é€Ÿåº¦: **10å€** é«˜é€ŸåŒ–

**å„ªå…ˆåº¦:** **ä¸­**

---

### 1.5 è©±è€…ãƒ€ã‚¤ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®ç²¾åº¦å‘ä¸Š

**ç¾çŠ¶:**
```python
# server/diarizer.py:31-101
class OnlineDiarizer:
    def assign(self, wav: np.ndarray) -> str:
        # ç°¡æ˜“çš„ãªã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ã¿
```

**æ”¹å–„æ¡ˆ:**
```python
from speechbrain.inference.speaker import SpeakerRecognition

class AdvancedDiarizer:
    def __init__(self):
        # ECAPA-TDNNãƒ™ãƒ¼ã‚¹ã®é«˜åº¦ãªè©±è€…èªè­˜
        self.recognition = SpeakerRecognition.from_hparams(
            source="speechbrain/spkrec-ecapa-voxceleb"
        )
        self.speaker_embeddings = {}  # è©±è€…ã”ã¨ã®åŸ‹ã‚è¾¼ã¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥

    def assign_speaker(self, wav: np.ndarray) -> str:
        embedding = self.recognition.encode_batch(wav)
        # æ—¢å­˜è©±è€…ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        best_match = self._find_best_speaker(embedding)
        if best_match and self.similarity(embedding, best_match) > 0.75:
            return best_match
        # æ–°è¦è©±è€…ã¨ã—ã¦ç™»éŒ²
        return self._register_new_speaker(embedding)
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- è©±è€…è­˜åˆ¥ç²¾åº¦: **85% â†’ 95%**
- æœ€å¤§è©±è€…æ•°: 2äºº â†’ 10äººå¯¾å¿œ

**å„ªå…ˆåº¦:** **ä¸­**

---

## 2. UI/UXæ”¹å–„

### 2.1 âœ… Vanilla JS â†’ React (Vite) ã¸ã®ç§»è¡Œ

**ç¾çŠ¶:**
```javascript
// web/main.js:1-965
// å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã§965è¡Œã€‚ä¿å®ˆæ€§ãŒä½ã„ã€‚
const $ = (q, scope = document) => scope.querySelector(q);
// ... ç”ŸDOMæ“ä½œã®é€£ç¶š
```

**æ”¹å–„æ¡ˆ:**
```jsx
// frontend/src/App.jsx
import { useState, useEffect, useRef } from 'react';
import { useAudioRecorder } from './hooks/useAudioRecorder';
import TranscriptList from './components/TranscriptList';
import Visualizer from './components/Visualizer';
import GeminiModal from './components/GeminiModal';

export default function App() {
  const [transcripts, setTranscripts] = useState([]);
  const [isRecording, setIsRecording] = useState(false);
  const { startRecording, stopRecording, audioLevel } = useAudioRecorder();

  return (
    <div className="min-h-screen bg-gray-50">
      <Header status={isRecording ? 'éŒ²éŸ³ä¸­' : 'å¾…æ©Ÿä¸­'} />
      <main className="max-w-4xl mx-auto p-4">
        <TranscriptList transcripts={transcripts} />
        <Visualizer level={audioLevel} />
      </main>
      <Footer
        isRecording={isRecording}
        onToggle={isRecording ? stopRecording : startRecording}
      />
      <GeminiModal transcripts={transcripts} />
    </div>
  );
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ãƒ»ä¿å®ˆæ€§: **å¤§å¹…å‘ä¸Š**
- ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å†åˆ©ç”¨æ€§: å®Ÿç¾
- å‹å®‰å…¨æ€§: TypeScriptå°å…¥ã§æ›´ã«å‘ä¸Š

**å„ªå…ˆåº¦:** **æœ€å„ªå…ˆ**

---

### 2.2 âœ… Tailwind CSSã«ã‚ˆã‚‹ãƒ¢ãƒ€ãƒ³ãƒ‡ã‚¶ã‚¤ãƒ³

**ç¾çŠ¶:**
```css
/* web/style.css */
/* ç”ŸCSSã§ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° */
.item { display: flex; align-items: flex-start; margin-bottom: 8px; }
.bubble { background: #f3f4f6; border-radius: 12px; padding: 8px 12px; }
```

**æ”¹å–„æ¡ˆ:**
```jsx
// frontend/src/components/TranscriptItem.jsx
export default function TranscriptItem({ transcript }) {
  return (
    <div className="flex items-start gap-3 mb-2 group">
      <div className="flex-shrink-0 w-16 text-sm text-gray-500 font-mono">
        {formatTime(transcript.tsStart)}
      </div>
      <div className="flex-1">
        {transcript.speaker && (
          <span className="inline-block px-2 py-0.5 text-xs font-semibold
                         bg-blue-100 text-blue-700 rounded-full mb-1">
            {transcript.speaker}
          </span>
        )}
        <p className="text-gray-800 leading-relaxed">
          {transcript.text}
        </p>
      </div>
      {/* ãƒ›ãƒãƒ¼ã§ç·¨é›†ãƒ»å‰Šé™¤ãƒœã‚¿ãƒ³è¡¨ç¤º */}
      <div className="opacity-0 group-hover:opacity-100 transition-opacity">
        <button className="p-1 hover:bg-gray-200 rounded">ç·¨é›†</button>
      </div>
    </div>
  );
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- ãƒ‡ã‚¶ã‚¤ãƒ³ã®ä¸€è²«æ€§: Tailwindã®è¨­è¨ˆã‚·ã‚¹ãƒ†ãƒ ã§ä¿è¨¼
- ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ: ãƒ¢ãƒã‚¤ãƒ«ã§ã‚‚å¿«é©ã«ä½¿ç”¨å¯èƒ½
- ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰: `dark:` ãƒ—ãƒªãƒ•ã‚£ã‚¯ã‚¹ã§ç°¡å˜å®Ÿè£…

**å„ªå…ˆåº¦:** **æœ€å„ªå…ˆ**

---

### 2.3 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼

**ç¾çŠ¶:**
```javascript
// web/main.js:717-799
// Canvas APIã§è‡ªä½œï¼ˆã‚³ã‚¹ãƒˆå¤§ï¼‰
function drawWave() {
    const ctx = waveCanvas.getContext('2d');
    // ... 80è¡Œã®æ³¢å½¢æç”»ãƒ­ã‚¸ãƒƒã‚¯
}
```

**æ”¹å–„æ¡ˆ:**
```jsx
// frontend/src/components/Visualizer.jsx
import { useEffect, useRef } from 'react';
import { AudioVisualizer } from 'react-audio-visualizers';

export default function Visualizer({ level, isRecording }) {
  const canvasRef = useRef(null);

  return (
    <div className="fixed bottom-20 left-1/2 -translate-x-1/2">
      {isRecording && (
        <AudioVisualizer
          ref={canvasRef}
          audioLevel={level}
          barWidth={4}
          barGap={2}
          barColor="#3b82f6"
          barCount={32}
          height={40}
        />
      )}
    </div>
  );
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- å®Ÿè£…ã‚³ã‚¹ãƒˆ: **80%** å‰Šæ¸›
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: WebGLã§GPUã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ãƒˆ
- ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³: 60fpsã§æ»‘ã‚‰ã‹ã«è¡¨ç¤º

**å„ªå…ˆåº¦:** **ä¸­**

---

### 2.4 ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®è­°äº‹éŒ²è¡¨ç¤º

**ç¾çŠ¶:**
```html
<!-- web/index.html -->
<div id="finalList">
  <div class="item">
    <div class="time">00:01</div>
    <div class="bubble">ç™ºè©±å†…å®¹</div>
  </div>
</div>
```

**æ”¹å–„æ¡ˆ:**
```jsx
// frontend/src/components/TranscriptList.jsx
export default function TranscriptList({ transcripts }) {
  return (
    <div className="space-y-4">
      {transcripts.map((t) => (
        <div
          key={t.id}
          className={cn(
            "flex gap-3 animate-in slide-in-from-bottom-2",
            t.isPartial && "opacity-70"
          )}
        >
          {/* ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§ãã®ä½ç½®ã¸ã‚¸ãƒ£ãƒ³ãƒ—ï¼‰ */}
          <button
            onClick={() => seekTo(t.tsStart)}
            className="flex-shrink-0 w-16 text-sm text-gray-400 hover:text-blue-600"
          >
            {formatTime(t.tsStart)}
          </button>

          {/* ç™ºè©±ãƒãƒ–ãƒ« */}
          <div className={cn(
            "flex-1 rounded-2xl px-4 py-2",
            t.speaker === 'A' ? "bg-blue-500 text-white" : "bg-gray-200 text-gray-800"
          )}>
            {t.speaker && (
              <span className="text-xs opacity-75 mb-1 block">
                {t.speaker}
              </span>
            )}
            <p className="text-sm leading-relaxed">
              {t.text}
            </p>
          </div>
        </div>
      ))}
    </div>
  );
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- è¦–èªæ€§: **2å€** å‘ä¸Š
- æ“ä½œæ€§: ã‚¯ãƒªãƒƒã‚¯ã§éŸ³å£°ä½ç½®ã¸ã‚¸ãƒ£ãƒ³ãƒ—
- æ‹¡å¼µæ€§: ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€çµµæ–‡å­—ç­‰ã®è¿½åŠ ãŒå®¹æ˜“

**å„ªå…ˆåº¦:** **é«˜**

---

### 2.5 Geminiçµæœã®ãƒ¢ãƒ¼ãƒ€ãƒ«è¡¨ç¤º

**æ–°è¦æ©Ÿèƒ½:**

```jsx
// frontend/src/components/GeminiModal.jsx
import { useState } from 'react';

export default function GeminiModal({ transcripts }) {
  const [result, setResult] = useState(null);
  const [isLoading, setIsLoading] = useState(false);

  const handleAnalyze = async () => {
    setIsLoading(true);
    const response = await fetch('/api/gemini/analyze', {
      method: 'POST',
      body: JSON.stringify({ transcripts }),
    });
    setResult(await response.json());
    setIsLoading(false);
  };

  return (
    <div className="fixed inset-0 bg-black/50 flex items-center justify-center">
      <div className="bg-white rounded-2xl shadow-2xl w-full max-w-2xl max-h-[80vh] overflow-auto">
        <header className="sticky top-0 bg-white border-b p-4 flex justify-between items-center">
          <h2 className="text-xl font-bold">è­°äº‹éŒ²åˆ†æ</h2>
          <button onClick={onClose}>âœ•</button>
        </header>

        <main className="p-6 space-y-6">
          {isLoading ? (
            <div className="flex justify-center">
              <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600" />
            </div>
          ) : result ? (
            <>
              <section>
                <h3 className="font-semibold text-gray-900 mb-2">ğŸ“ è¦ç´„</h3>
                <p className="text-gray-700">{result.summary}</p>
              </section>

              <section>
                <h3 className="font-semibold text-gray-900 mb-2">âœ… ã‚¿ã‚¹ã‚¯</h3>
                <ul className="space-y-2">
                  {result.tasks.map((task) => (
                    <li key={task.id} className="flex items-center gap-2">
                      <input type="checkbox" className="rounded" />
                      <span>{task.text}</span>
                      <span className="text-xs text-gray-500">{task.assignee}</span>
                    </li>
                  ))}
                </ul>
              </section>

              <section>
                <h3 className="font-semibold text-gray-900 mb-2">ğŸ’¡ æ±ºå®šäº‹é …</h3>
                <ul className="list-disc list-inside text-gray-700">
                  {result.decisions.map((d) => <li key={d}>{d}</li>)}
                </ul>
              </section>
            </>
          ) : (
            <div className="text-center text-gray-500">
              <p>ã€Œåˆ†æã€ãƒœã‚¿ãƒ³ã§GeminiãŒè­°äº‹éŒ²ã‚’è¦ç´„ã—ã¾ã™</p>
              <button
                onClick={handleAnalyze}
                className="mt-4 px-6 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700"
              >
                åˆ†æé–‹å§‹
              </button>
            </div>
          )}
        </main>
      </div>
    </div>
  );
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- è­°äº‹éŒ²ã®è³ª: **Gemini 2.5** ã§ãƒ—ãƒ­å“è³ªã«
- ã‚¿ã‚¹ã‚¯æ¼ã‚Œ: **90%** å‰Šæ¸›
- ä¼šè­°æ™‚é–“: **20%** çŸ­ç¸®ï¼ˆè¦ç´„ä½œæˆã®æ‰‹é–“ã‚’çœãï¼‰

**å„ªå…ˆåº¦:** **é«˜**

---

## 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 3.1 H200 GPUå‘ã‘æœ€é©åŒ–

**ç¾çŠ¶:**
```python
# GPUãƒ¡ãƒ¢ãƒªãŒåŠ¹ç‡çš„ã«ä½¿ã‚ã‚Œã¦ã„ãªã„å¯èƒ½æ€§
model = model.to("cuda")
```

**æ”¹å–„æ¡ˆ:**
```python
# H200ã®28GB HBM3ã‚’æœ€å¤§æ´»ç”¨
class H200OptimizedTranscriber:
    def __init__(self):
        # ãƒ¢ãƒ‡ãƒ«ã‚’åŠç²¾åº¦ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªåŠæ¸›ï¼‰
        self.model = WhisperModel(
            "large-v3",
            device="cuda",
            compute_type="float16",
            # Flash Attention 2æœ‰åŠ¹åŒ–ï¼ˆH200ã§é«˜é€ŸåŒ–ï¼‰
            attn_implementation="flash_attention_2",
            # TensorRTæœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            use_tensorrt=True,
        )
        # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒãƒ
        torch.cuda.set_per_process_memory_fraction(0.9)
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- æ¨è«–é€Ÿåº¦: **2å€** é«˜é€ŸåŒ–
- ãƒãƒƒãƒå‡¦ç†: åŒæ™‚æ¥ç¶š **10ã‚»ãƒƒã‚·ãƒ§ãƒ³** ã¾ã§å¯¾å¿œ
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨åŠ¹ç‡: **30%** å‘ä¸Š

**å„ªå…ˆåº¦:** **é«˜**

---

### 3.2 ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°æˆ¦ç•¥ã®æœ€é©åŒ–

**ç¾çŠ¶:**
```python
# å›ºå®šãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã§å¾…æ©Ÿæ™‚é–“ãŒç™ºç”Ÿ
BUFFER_SIZE = 2000  # ms
```

**æ”¹å–„æ¡ˆ:**
```python
class AdaptiveBuffer:
    def __init__(self):
        self.min_buffer = 500   # ms
        self.max_buffer = 3000  # ms
        self.target_latency = 800  # ms
        self.current_buffer = 1000

    def adjust_buffer(self, processing_time: float):
        # å‡¦ç†æ™‚é–“ã«å¿œã˜ã¦ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’å‹•èª¿æ•´
        if processing_time > self.target_latency:
            self.current_buffer = min(self.max_buffer, self.current_buffer * 1.1)
        else:
            self.current_buffer = max(self.min_buffer, self.current_buffer * 0.95)
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: **500ms** ä»¥ä¸‹ï¼ˆç›®æ¨™ï¼‰
- å‡¦ç†é…å»¶ã®å¤‰å‹•: **50%** ä½æ¸›

**å„ªå…ˆåº¦:** **ä¸­**

---

### 3.3 WebSocketé€šä¿¡ã®æœ€é©åŒ–

**ç¾çŠ¶:**
```javascript
// web/main.js:693-703
// ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥é€ä¿¡ï¼ˆéåŠ¹ç‡ï¼‰
const header = new ArrayBuffer(8);
const body = payload;
const out = new Uint8Array(header.byteLength + body.byteLength);
ws.send(out);
```

**æ”¹å–„æ¡ˆ:**
```javascript
// åœ§ç¸®+ãƒãƒƒãƒå‡¦ç†
class OptimizedAudioSender {
  constructor(ws) {
    this.ws = ws;
    this.batch = [];
    this.batchTimer = null;
  }

  send(audioData) {
    this.batch.push(audioData);

    // 100msåˆ†ã‚’ãƒãƒƒãƒå‡¦ç†
    if (!this.batchTimer) {
      this.batchTimer = setTimeout(() => {
        this.flush();
      }, 100);
    }
  }

  flush() {
    // Deflateåœ§ç¸®ã‚’é©ç”¨
    const compressed = pako.deflate(JSON.stringify(this.batch));
    this.ws.send(compressed);
    this.batch = [];
    this.batchTimer = null;
  }
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸ: **60%** å‰Šæ¸›
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: **100ms** æ”¹å–„

**å„ªå…ˆåº¦:** **ä¸­**

---

## 4. ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆï¼ˆvLLMï¼‰

### 4.1 vLLM + Llama 3.1 70B / Qwen2.5 72B ã®å°å…¥

**æ–°è¦æ©Ÿèƒ½:**

```python
# backend/llm_service.py
from vllm import LLM, SamplingParams
from typing import List, Dict
import json

class LocalLLMService:
    def __init__(self, model_path: str = "meta-llama/Llama-3.1-70B-Instruct"):
        # vLLMã®åˆæœŸåŒ–ï¼ˆH200å‘ã‘æœ€é©åŒ–ï¼‰
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=1,           # H200ã‚·ãƒ³ã‚°ãƒ«GPU
            max_model_len=32768,              # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·
            trust_remote_code=True,
            dtype="bfloat16",                 # H200ã§æœ€é©ãªç²¾åº¦
            gpu_memory_utilization=0.9,       # GPUãƒ¡ãƒ¢ãƒª90%ä½¿ç”¨
            enable_prefix_caching=True,       # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§é«˜é€ŸåŒ–
        )

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.sampling_params = SamplingParams(
            temperature=0.3,
            top_p=0.9,
            max_tokens=2048,
            stop=["<|end_of_text|>", "<|eot_id|>"]
        )

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.system_prompt = """ã‚ãªãŸã¯å„ªç§€ãªè­°äº‹éŒ²ä½œæˆè€…ã§ã™ã€‚
ä¼šè­°å†…å®¹ã‹ã‚‰ä»¥ä¸‹ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
1. è¦ç´„ï¼ˆ3æ®µè½ç¨‹åº¦ã€ç®‡æ¡æ›¸ãï¼‰
2. ã‚¿ã‚¹ã‚¯ä¸€è¦§ï¼ˆæ‹…å½“è€…å«ã‚€ï¼‰
3. æ±ºå®šäº‹é …

å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ï¼š
{
  "summary": "...",
  "tasks": [{"text": "...", "assignee": "..."}],
  "decisions": ["...", "..."]
}
"""

    def analyze(self, transcripts: List[Dict]) -> Dict:
        """è­°äº‹éŒ²ã‚’åˆ†æ"""
        # ç›´è¿‘ã®è­°äº‹éŒ²ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
        text = "\n".join([
            f"[{t['timestamp']}] {t.get('speaker', '')}: {t['text']}"
            for t in transcripts
        ])

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆLlama 3.1ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{self.system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nä¼šè­°å†…å®¹:\n{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

        # æ¨è«–å®Ÿè¡Œ
        outputs = self.llm.generate([prompt], self.sampling_params)

        # JSONãƒ‘ãƒ¼ã‚¹
        result_text = outputs[0].outputs[0].text.strip()
        try:
            return json.loads(result_text)
        except json.JSONDecodeError:
            # JSONãƒãƒ¼ã‚«ãƒ¼ã§æŠ½å‡º
            start = result_text.find('{')
            end = result_text.rfind('}') + 1
            return json.loads(result_text[start:end])

    async def analyze_streaming(self, transcripts: List[Dict]):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§åˆ†æçµæœã‚’è¿”ã™"""
        text = "\n".join([
            f"[{t['timestamp']}] {t.get('speaker', '')}: {t['text']}"
            for t in transcripts
        ])

        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{self.system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nä¼šè­°å†…å®¹:\n{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
        from vllm.engine.arg_utils import AsyncEngineArgs
        from vllm.engine.async_llm_engine import AsyncLLMEngine

        engine = AsyncLLMEngine.from_engine_args(
            AsyncEngineArgs(
                model="meta-llama/Llama-3.1-70B-Instruct",
                tensor_parallel_size=1,
                dtype="bfloat16",
                gpu_memory_utilization=0.9,
            )
        )

        async for request_output in engine.generate(prompt, self.sampling_params):
            yield request_output.outputs[0].text
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- è­°äº‹éŒ²ä½œæˆæ™‚é–“: **30åˆ† â†’ 2åˆ†** ã«çŸ­ç¸®ï¼ˆGemini APIã‚ˆã‚Šé«˜é€Ÿï¼‰
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: **<3ç§’**ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–ã®æ©æµï¼‰
- ã‚³ã‚¹ãƒˆ: **$0**ï¼ˆAPIä¸è¦ï¼‰
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: **å®Œå…¨ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹**ï¼ˆãƒ‡ãƒ¼ã‚¿å¤–æµãªã—ï¼‰

**å„ªå…ˆåº¦:** **æœ€å„ªå…ˆ**

---

### 4.2 H200å‘ã‘vLLMãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**æœ€é©åŒ–:**

```python
# backend/vllm_config.py
from vllm import LLM

# H200 (28GB HBM3) å‘ã‘æœ€é©è¨­å®š
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",

    # === ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– ===
    tensor_parallel_size=1,           # H200ã‚·ãƒ³ã‚°ãƒ«ã§å®Œçµ
    dtype="bfloat16",                 # H200ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆ
    gpu_memory_utilization=0.9,       # 28GBã®90%ã‚’ä½¿ç”¨

    # === ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ– ===
    enable_prefix_caching=True,       # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    max_num_seqs=16,                  # æœ€å¤§16ãƒãƒƒãƒå‡¦ç†
    use_v2_block_manager=True,        # ãƒ¡ãƒ¢ãƒªç®¡ç†v2ã§é«˜é€ŸåŒ–

    # === H200ç‰¹åŒ– ===
    enforce_eager=True,               # CUDA Graphæœ€é©åŒ–
    kv_cache_dtype="fp8",             # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’FP8ã§çœãƒ¡ãƒ¢ãƒª
)
```

**æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ï¼ˆH200ã§å‹•ä½œï¼‰ï¼š**

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | VRAMä½¿ç”¨ | ç²¾åº¦ | ç‰¹å¾´ |
|--------|-----------|----------|------|------|
| **Qwen2.5 72B Instruct** | 72B | ~24GB | æœ€é«˜ | æ—¥æœ¬èªæœ€å¼·ã€æ•°å­¦ãƒ»ã‚³ãƒ¼ãƒ‰ã«å¼·ã„ |
| **Llama 3.1 70B Instruct** | 70B | ~22GB | æœ€é«˜ | è‹±èªåœæ¨™æº–ã€æ±ç”¨æ€§é«˜ã„ |
| **Qwen2.5 32B Instruct** | 32B | ~12GB | é«˜ | è»½é‡ã€é«˜é€Ÿï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é‡è¦–ï¼‰ |
| **Llama 3.1 8B Instruct** | 8B | ~4GB | ä¸­ | è¶…è»½é‡ã€Whisperã¨åŒæ™‚å®Ÿè¡Œå¯ |

**æ¨å¥¨:** Qwen2.5 72Bï¼ˆæ—¥æœ¬èªã®è­°äº‹éŒ²å“è³ªãŒæœ€é«˜ï¼‰

**å„ªå…ˆåº¦:** **é«˜**

---

### 4.3 OpenAI Compatible APIã§ã®çµ±åˆ

**å®Ÿè£…ä¾‹:**

```python
# backend/server.py
from fastapi import FastAPI
from openai import OpenAI

# vLLMã®OpenAI Compatible APIã‚’èµ·å‹•
# åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-72B-Instruct

app = FastAPI()

# ãƒ­ãƒ¼ã‚«ãƒ«vLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«æ¥ç¶š
client = OpenAI(
    base_url="http://localhost:8000/v1",  # vLLMã‚µãƒ¼ãƒãƒ¼
    api_key="dummy"  # èªè¨¼ãªã—
)

@app.post("/api/llm/analyze")
async def analyze_transcripts(transcripts: List[Dict]):
    response = client.chat.completions.create(
        model="Qwen/Qwen2.5-72B-Instruct",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": format_transcripts(transcripts)}
        ],
        temperature=0.3,
        response_format={"type": "json_object"}
    )
    return json.loads(response.choices[0].message.content)
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- å®Ÿè£… simplicity: OpenAI SDKãã®ã¾ã¾ä½¿ãˆã‚‹
- æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®äº’æ›æ€§: 100%
- ãƒ‡ãƒãƒƒã‚°å®¹æ˜“æ€§: Curlç­‰ã§ç›´æ¥ãƒ†ã‚¹ãƒˆå¯èƒ½

**å„ªå…ˆåº¦:** **ä¸­**

---

## 5. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ·æ–°

### 5.1 âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã®å†ç·¨

**ç¾çŠ¶:**
```
whistx/
â”œâ”€â”€ server/          # Pythonãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
â”œâ”€â”€ web/             # Vanilla JSãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
â””â”€â”€ docs/            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
```

**æ”¹å–„æ¡ˆ:**
```
whistx/
â”œâ”€â”€ backend/                 # Pythonãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆH200åˆ¶å¾¡ï¼‰
â”‚   â”œâ”€â”€ venv/                # ä»®æƒ³ç’°å¢ƒ
â”‚   â”œâ”€â”€ server.py            # WebSocketã‚µãƒ¼ãƒãƒ¼
â”‚   â”œâ”€â”€ transcription.py     # Whisperæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
â”‚   â”œâ”€â”€ llm_service.py       # vLLMãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆ
â”‚   â”œâ”€â”€ vllm_config.py       # vLLMè¨­å®š
â”‚   â”œâ”€â”€ utils.py             # éŸ³å£°å‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â””â”€â”€ requirements.txt     # ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
â”‚
â”œâ”€â”€ models/                  # ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚­ãƒ£ãƒƒã‚·ãƒ¥
â”‚   â”œâ”€â”€ Qwen2.5-72B-Instruct/    # HuggingFaceã‹ã‚‰è‡ªå‹•DL
â”‚   â””â”€â”€ large-v3/                # Whisperãƒ¢ãƒ‡ãƒ«
â”‚
â”œâ”€â”€ frontend/                # Reactã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ï¼ˆUIï¼‰
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/      # UIãƒ‘ãƒ¼ãƒ„
â”‚   â”‚   â”œâ”€â”€ hooks/           # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
â”‚   â”‚   â”œâ”€â”€ lib/             # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”‚   â”œâ”€â”€ App.jsx          # ãƒ¡ã‚¤ãƒ³ç”»é¢
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ tests/                   # ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ backend/
â”‚   â””â”€â”€ frontend/
â”‚
â””â”€â”€ README.md
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰/ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®åˆ†é›¢: é–‹ç™ºåŠ¹ç‡**2å€**
- ãƒ‡ãƒ—ãƒ­ã‚¤: å„ã€…ã‚’ç‹¬ç«‹ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½

**å„ªå…ˆåº¦:** **é«˜**

---

### 5.2 âœ… é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®è¨­è¨ˆ

**æ–°è¦è¨­è¨ˆ:**

```python
# backend/server.py
import asyncio
import websockets
from typing import Literal

async def handle_connection(websocket):
    """WebSocketãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æŒ¯ã‚Šåˆ†ã‘"""
    async for message in websocket:
        # ãƒã‚¤ãƒŠãƒª = éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        if isinstance(message, bytes):
            await handle_audio(websocket, message)

        # ãƒ†ã‚­ã‚¹ãƒˆ = JSONã‚³ãƒãƒ³ãƒ‰
        else:
            cmd = json.loads(message)
            if cmd["type"] == "start":
                await handle_start(websocket, cmd["opts"])
            elif cmd["type"] == "analyze":
                await handle_llm_analyze(websocket, cmd["sessionId"])
            elif cmd["type"] == "stop":
                await handle_stop(websocket)

async def handle_audio(websocket, audio_data: bytes):
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Whisperã§å‡¦ç†"""
    result = await transcriber.transcribe(audio_data)
    await websocket.send(json.dumps({
        "type": "transcript",
        "data": result
    }))

async def handle_llm_analyze(websocket, session_id: str):
    """vLLMã§è­°äº‹éŒ²ã‚’åˆ†æ"""
    transcripts = await get_transcripts(session_id)
    analysis = await llm_service.analyze(transcripts)
    await websocket.send(json.dumps({
        "type": "llm_result",
        "data": analysis
    }))
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
- é€šä¿¡åŠ¹ç‡: ãƒã‚¤ãƒŠãƒª/ãƒ†ã‚­ã‚¹ãƒˆã§æœ€é©åŒ–
- æ‹¡å¼µæ€§: æ–°ã‚³ãƒãƒ³ãƒ‰ã‚’å®¹æ˜“ã«è¿½åŠ 

**å„ªå…ˆåº¦:** **é«˜**

---

## 6. å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 1: åŸºç›¤æ§‹ç¯‰ï¼ˆ2é€±é–“ï¼‰

| ã‚¿ã‚¹ã‚¯ | æœŸæ—¥ | æ‹…å½“ | çŠ¶æ…‹ |
|--------|------|------|------|
| Reactãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | Day 1-2 | Frontend | âœ… å®Œäº† |
| Tailwind CSSã®å°å…¥ & ãƒ‡ã‚¶ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ | Day 3-4 | Frontend | âœ… å®Œäº† |
| ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†ç·¨ | Day 1-2 | Backend | âœ… å®Œäº† |
| Whisper Large-v3 (Float16) ã®å°å…¥ | Day 3-5 | Backend | âœ… å®Œäº† |
| H200 GPUç’°å¢ƒã®æ§‹ç¯‰ | Day 5-7 | Infra | ğŸ”„ é€²è¡Œä¸­ |

**ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³:** React + Whisperã§åŸºæœ¬å‹•ä½œã‚’ç¢ºèª

---

### Phase 2: ã‚³ã‚¢æ©Ÿèƒ½å®Ÿè£…ï¼ˆ3é€±é–“ï¼‰

| ã‚¿ã‚¹ã‚¯ | æœŸæ—¥ | æ‹…å½“ | çŠ¶æ…‹ |
|--------|------|------|------|
| ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚­ãƒ£ãƒ—ãƒãƒ£ã®å®Ÿè£…ï¼ˆãƒã‚¤ã‚¯/ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°å¯¾å¿œï¼‰ | Day 8-11 | Frontend | âœ… å®Œäº† |
| WebSocketé€šä¿¡ã®å®Ÿè£… | Day 11-13 | Both | âœ… å®Œäº† |
| è­°äº‹éŒ²è¡¨ç¤ºï¼ˆãƒãƒ£ãƒƒãƒˆå½¢å¼ï¼‰ | Day 14-16 | Frontend | âœ… å®Œäº† |
| VADã®é«˜åº¦åŒ– | Day 14-16 | Backend | âœ… å®Œäº† |
| Initial Promptã«ã‚ˆã‚‹æ–‡è„ˆç¶­æŒ | Day 17-18 | Backend | âœ… å®Œäº† |
| ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼ã®å®Ÿè£… | Day 19-20 | Frontend | âœ… å®Œäº† |

**ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³:** ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§æ–‡å­—èµ·ã“ã—ãŒå‹•ä½œ - ğŸŸ¢ **é”æˆ**

---

### Phase 3: ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆï¼ˆ2é€±é–“ï¼‰

| ã‚¿ã‚¹ã‚¯ | æœŸæ—¥ | æ‹…å½“ | çŠ¶æ…‹ |
|--------|------|------|------|
| vLLMã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | Day 21-22 | Backend | âœ… å®Œäº† |
| Qwen2.5 72B / Llama 3.1 70B ã®DL & æ¤œè¨¼ | Day 23-24 | Backend | ğŸ”„ é€²è¡Œä¸­ |
| LLM Serviceã®å®Ÿè£… | Day 25-27 | Backend | âœ… å®Œäº† |
| H200å‘ã‘ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | Day 27-28 | Backend | â¸ï¸ GPUç’°å¢ƒå¾…ã¡ |
| ãƒ¢ãƒ¼ãƒ€ãƒ«UIã®å®Ÿè£… | Day 29-30 | Frontend | âœ… å®Œäº† |

**ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³:** ãƒ­ãƒ¼ã‚«ãƒ«LLMåˆ†ææ©Ÿèƒ½ãŒå®Œäº† - ğŸ”„ å®Ÿè£…ä¸­

---

### Phase 4: æœ€é©åŒ– & ãƒ†ã‚¹ãƒˆï¼ˆ2é€±é–“ï¼‰

| ã‚¿ã‚¹ã‚¯ | æœŸæ—¥ | æ‹…å½“ |
|--------|------|------|
| H200 GPUã®æ€§èƒ½ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | Day 31-33 | Backend |
| ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆæ¸¬ & æœ€é©åŒ– | Day 34-35 | Both |
| E2Eãƒ†ã‚¹ãƒˆã®å®Ÿè£… | Day 36-38 | QA |
| UI/UXã®å¾®èª¿æ•´ | Day 39-40 | Frontend |
| ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ | Day 41-42 | Docs |

**ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³:** ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†

---

## å„ªå…ˆåº¦ã‚µãƒãƒª

| å„ªå…ˆåº¦ | é …ç›®æ•° | ä¸»ãªæ”¹å–„ç‚¹ |
|--------|--------|------------|
| **æœ€å„ªå…ˆ** | 5 | Whisper Large-v3ã€ReactåŒ–ã€Tailwind CSSã€H200æœ€é©åŒ–ã€vLLMçµ±åˆ |
| **é«˜** | 8 | Initial Promptã€é«˜åº¦ãªVADã€ãƒãƒ£ãƒƒãƒˆå½¢å¼UIã€ãƒ¢ãƒ¼ãƒ€ãƒ«ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ·æ–°ç­‰ |
| **ä¸­** | 6 | ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼ã€ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°æœ€é©åŒ–ã€è©±è€…ãƒ€ã‚¤ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ |

---

## æˆåŠŸæŒ‡æ¨™ï¼ˆKPIï¼‰

| æŒ‡æ¨™ | ç¾çŠ¶ | ç›®æ¨™ | æ¸¬å®šæ–¹æ³• |
|------|------|------|----------|
| æ—¥æœ¬èªWER | ã€œ20% | **<5%** | æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡ |
| å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ã€œ1.5ç§’ | **<0.5ç§’** | éŸ³å£°å…¥åŠ›â†’è¡¨ç¤ºã¾ã§ã®æ™‚é–“ |
| LLMåˆ†æãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | N/A | **<3ç§’** | vLLMãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–æ™‚é–“ |
| æœ€å¤§åŒæ™‚æ¥ç¶šæ•° | ã€œ3ã‚»ãƒƒã‚·ãƒ§ãƒ³ | **10ã‚»ãƒƒã‚·ãƒ§ãƒ³** | H200ã§ã®è² è·ãƒ†ã‚¹ãƒˆ |
| è­°äº‹éŒ²ä½œæˆæ™‚é–“ | 30åˆ† | **2åˆ†** | vLLMåˆ†æå®Œäº†ã¾ã§ |
| UIãƒ¬ã‚¹ãƒãƒ³ã‚¹ | ã€œ200ms | **<50ms** | React DevTools Profiler |
| APIã‚³ã‚¹ãƒˆ | $0/æœˆ | **$0/æœˆ** | å®Œå…¨ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ |

---

## ç²¾åº¦æ‹…ä¿ã®ä»•çµ„ã¿

### å¤šå±¤çš„ãªç²¾åº¦å‘ä¸Šã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

Whistx v2ã§ã¯ä»¥ä¸‹ã®å¤šå±¤çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æ–‡å­—èµ·ã“ã—ç²¾åº¦ã‚’æ‹…ä¿ã—ã¦ã„ã¾ã™ï¼š

#### 1. ãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒ™ãƒ«ã®ç²¾åº¦å‘ä¸Š

| æŠ€è¡“ | åŠ¹æœ | å®Ÿè£…çŠ¶æ…‹ |
|------|------|----------|
| **Whisper Large-v3** | WER 5% (æ¥­ç•Œæœ€é«˜æ°´æº–) | âœ… å®Ÿè£…æ¸ˆ |
| **Float16ç²¾åº¦** | H200ã§æœ€é©ãªç²¾åº¦ã¨é€Ÿåº¦ | âœ… å®Ÿè£…æ¸ˆ |
| **99è¨€èªå¯¾å¿œ** | å¤šè¨€èªæ··åœ¨ä¼šè©±ã«ã‚‚å¯¾å¿œ | âœ… å®Ÿè£…æ¸ˆ |

#### 2. æ–‡è„ˆãƒ¬ãƒ™ãƒ«ã®ç²¾åº¦å‘ä¸Š

| æŠ€è¡“ | åŠ¹æœ | å®Ÿè£…çŠ¶æ…‹ |
|------|------|----------|
| **Initial Prompt** | å°‚é–€ç”¨èªèªè­˜+30% | âœ… `TranscriptionContext` |
| **ä¼šè©±å±¥æ­´ç®¡ç†** | ç›´è¿‘10ç™ºè©±ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŒ– | âœ… å®Ÿè£…æ¸ˆ |
| **è©±è€…ãƒ€ã‚¤ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³** | è©±è€…ã”ã¨ã®æ–‡è„ˆç¶­æŒ | ğŸ”„ Phase 4äºˆå®š |

#### 3. ä¿¡å·å‡¦ç†ãƒ¬ãƒ™ãƒ«ã®ç²¾åº¦å‘ä¸Š

| æŠ€è¡“ | åŠ¹æœ | å®Ÿè£…çŠ¶æ…‹ |
|------|------|----------|
| **Silero VAD** | é«˜ç²¾åº¦éŸ³å£°åŒºé–“æ¤œå‡º | âœ… `AdaptiveVAD` |
| **ç’°å¢ƒãƒã‚¤ã‚ºã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | é¨’éŸ³ç’°å¢ƒã§ã®èª¤æ¤œçŸ¥ä½æ¸› | âœ… å®Ÿè£…æ¸ˆ |
| **48kHzâ†’16kHzãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | é«˜å“è³ªãªéŸ³å£°å¤‰æ› | âœ… AudioWorklet |

#### 4. æ¨è«–ãƒ¬ãƒ™ãƒ«ã®ç²¾åº¦å‘ä¸Š

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | è¨­å®šå€¤ | åŠ¹æœ |
|----------|--------|------|
| `beam_size` | 12 | ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã§æœ€é©è§£æ¢ç´¢ |
| `temperature` | 0.0 | ç¢ºå®šçš„å‡ºåŠ›ã§å®‰å®šæ€§å‘ä¸Š |
| `best_of` | 5 | 5ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰æœ€è‰¯ã‚’é¸æŠ |
| `vad_filter` | True | VADã§ç„¡éŸ³åŒºé–“ã‚’é™¤å» |

#### 5. VRAMåŠ¹ç‡ã¨ä¸¦åˆ—å‡¦ç†

| æŠ€è¡“ | åŠ¹æœ | å®Ÿè£…çŠ¶æ…‹ |
|------|------|----------|
| **ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰** | VRAMåŠ¹ç‡åŒ– | âœ… ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ |
| **ä¸¦åˆ—å‡¦ç†å¯¾å¿œ** | è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼åŒæ™‚å‡¦ç† | âœ… ãƒ­ãƒƒã‚¯ãªã— |
| **vLLMè‡ªå‹•ãƒãƒƒãƒå‡¦ç†** | LLMæ¨è«–ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š | âœ… å®Ÿè£…æ¸ˆ |

### ç²¾åº¦æ¸¬å®šã¨è©•ä¾¡

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡

```bash
# æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
python benchmark/transcribe.py \
  --model large-v3 \
  --dataset librispeech \
  --language ja

# æœŸå¾…çµæœ
# - æ—¥æœ¬èª WER: < 5%
# - è‹±èª WER: < 2%
# - å‡¦ç†é€Ÿåº¦: RTF < 0.1 (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ )
```

#### å®Ÿé‹ç”¨ã§ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```bash
# ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
curl http://localhost:8005/api/status

# æœŸå¾…ã•ã‚Œã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹
{
  "status": "running",
  "active_connections": 3,
  "whisper_model": "large-v3",
  "llm_model": "Qwen/Qwen2.5-7B-Instruct"
}
```

### ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

ç’°å¢ƒå¤‰æ•°ã§èª¿æ•´å¯èƒ½ï¼š

```bash
# Whisperãƒ¢ãƒ‡ãƒ«ï¼ˆç²¾åº¦ vs é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
WHISPER_MODEL=large-v3  # tiny < small < medium < large-v3

# æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
WHISPER_TEMPERATURE=0.0  # 0=ç¢ºå®šæ€§ã€0.1-0.3ã§å¤šæ§˜æ€§
WHISPER_BEAM_SIZE=12     # 5-15ã€å¤§ãã„ã»ã©ç²¾åº¦å‘ä¸Š

# VADæ„Ÿåº¦
VAD_THRESHOLD=0.5        # 0.1-0.9ã€ç’°å¢ƒã«åˆã‚ã›ã¦èª¿æ•´
```

---

## çµè«–

Whistx v2ã§ã¯ã€**èªè­˜ç²¾åº¦**ã¨**UI/UX**ã‚’è»¸ã«ä»¥ä¸‹ã®é©æ–°ã‚’ç›®æŒ‡ã—ã¾ã™ï¼š

### ç²¾åº¦é¢ã§ã®é©æ–°
1. **Whisper Large-v3 (Float16)** ã§æ¥­ç•Œæœ€é«˜æ°´æº–ã®ç²¾åº¦ã‚’å®Ÿç¾
2. **Initial Prompt** ã§å°‚é–€ç”¨èªã®èªè­˜ç‡ã‚’30%å‘ä¸Š
3. **H200 GPU** ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’ç¶­æŒã—ãŸã¾ã¾é«˜ç²¾åº¦åŒ–

### UIé¢ã§ã®é©æ–°
1. **React + Tailwind** ã§ãƒ¢ãƒ€ãƒ³ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’æä¾›
2. **ãƒãƒ£ãƒƒãƒˆå½¢å¼** ã§è¦–èªæ€§ã‚’2å€å‘ä¸Š
3. **vLLM + Qwen2.5 72B** ã§è­°äº‹éŒ²ä½œæˆæ™‚é–“ã‚’30åˆ†ã‹ã‚‰2åˆ†ã«çŸ­ç¸®

### ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹åŒ–ã®ãƒ¡ãƒªãƒƒãƒˆ
1. **ã‚³ã‚¹ãƒˆ:** APIå‘¼ã³å‡ºã—è²»ãŒ**å®Œå…¨ã«$0**
2. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£:** ä¼šè­°ãƒ‡ãƒ¼ã‚¿ãŒç¤¾å¤–ã«**ä¸€åˆ‡å‡ºãªã„**
3. **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·:** ãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–ã§**<3ç§’**ã®é«˜é€Ÿå¿œç­”
4. **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º:** è‡ªç¤¾å°‚é–€ç”¨èªã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½

ã“ã‚Œã«ã‚ˆã‚Šã€Whistxã¯ã€Œæ–‡å­—èµ·ã“ã—ãƒ„ãƒ¼ãƒ«ã€ã‹ã‚‰ã€Œ**å®Œå…¨ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ä¼šè­°ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **ã€ã¸é€²åŒ–ã—ã¾ã™ã€‚
